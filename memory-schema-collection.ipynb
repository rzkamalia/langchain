{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from trustcall import create_extractor\n",
    "\n",
    "from langchain_core.messages import merge_message_runs, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,        # this allows the extractor to insert new memories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SYSTEM_MESSAGE = \"\"\"\n",
    "    You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "    You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "    Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "    {memory}\n",
    "\"\"\"\n",
    "\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"\n",
    "    Reflect on following interaction. \n",
    "\n",
    "    Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "    Use parallel tool calling to handle updates and insertions simultaneously:\n",
    "\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memory = (\n",
    "        [\n",
    "            (existing_item.key, tool_name, existing_item.value)\n",
    "            for existing_item in existing_items\n",
    "        ]\n",
    "            if existing_items\n",
    "            else None\n",
    "    )\n",
    "\n",
    "    # merge the chat history and the instruction\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "\n",
    "    result = trustcall_extractor.invoke(\n",
    "        {\n",
    "            \"messages\": updated_messages, \n",
    "            \"existing\": existing_memory\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"call_model\", call_model)\n",
    "graph.add_node(\"write_memory\", write_memory)\n",
    "graph.add_edge(START, \"call_model\")\n",
    "graph.add_edge(\"call_model\", \"write_memory\")\n",
    "graph.add_edge(\"write_memory\", END)\n",
    "\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Yoona\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Yoona! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Yoona\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I love 2nd generation k-pop group and AI enthusiast.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great to hear, Yoona! It's cool that you're a fan of 2nd generation K-pop groups and interested in AI. Is there a specific group or AI topic you'd like to discuss today?\n"
     ]
    }
   ],
   "source": [
    "input_messages = [HumanMessage(content=\"I love 2nd generation k-pop group and AI enthusiast.\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Yoona\"}, 'key': 'a4e0fb28-1968-48e2-bdf8-d0f19a94db54', 'namespace': ['memories', '1'], 'created_at': '2025-01-01T10:18:34.832644+00:00', 'updated_at': '2025-01-01T10:18:34.832645+00:00', 'score': None}\n",
      "{'value': {'content': 'User loves 2nd generation K-pop group and is an AI enthusiast.'}, 'key': 'dcb5ea3a-8976-42fb-935a-62e73eee3cd2', 'namespace': ['memories', '1'], 'created_at': '2025-01-01T10:18:36.677389+00:00', 'updated_at': '2025-01-01T10:18:36.677394+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What book about AI do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I recommend \"Life 3.0: Being Human in the Age of Artificial Intelligence\" by Max Tegmark. It's a thought-provoking book that explores the impact of AI on our future society and raises important questions about the development of artificial intelligence. It should be an interesting read for you as an AI enthusiast!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "input_messages = [HumanMessage(content=\"What book about AI do you recommend for me?\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
