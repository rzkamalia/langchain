{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference between memory-schema-profile-1 and memory-schema-profile-2 is in memory-schema-profile-1 using with_structured_output to invoke the model. in memory-schema-profile-2, we will use original invoke and use TrustCall. TrustCall is an open-source library for updating JSON schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from trustcall import create_extractor\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_vmlqGOnGnIRTXP2mzOZEPiOI)\n",
      " Call ID: call_vmlqGOnGnIRTXP2mzOZEPiOI\n",
      "  Args:\n",
      "    user_name: Yoona\n",
      "    interests: ['2nd generation k-pop group', 'AI enthusiast']\n"
     ]
    }
   ],
   "source": [
    "# example using TrustCall\n",
    "\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, i am Yoona.\"), \n",
    "    AIMessage(content=\"Nice to meet you, Yoona.\"), \n",
    "    HumanMessage(content=\"I love 2nd generation k-pop group and AI enthusiast.\")\n",
    "]\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"User profile schema with typed fields.\n",
    "    \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    interests: List[str] = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\"\n",
    ")\n",
    "\n",
    "system_msg = \"Extract the user profile from the following conversation\"\n",
    "\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})\n",
    "\n",
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    \"\"\"Profile of a user.\n",
    "    \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SYSTEM_MESSAGE = \"\"\"\n",
    "    You are a helpful assistant with memory that provides information about the user. \n",
    "    If you have memory for this user, use it to personalize your responses.\n",
    "    Here is the memory (it may be empty): {memory}\n",
    "\"\"\"\n",
    "\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"\n",
    "    Create or update the memory (JSON doc) to incorporate information from the following conversation:\n",
    "\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"      \n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "    \n",
    "    result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "    \n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"call_model\", call_model)\n",
    "graph.add_node(\"write_memory\", write_memory)\n",
    "graph.add_edge(START, \"call_model\")\n",
    "graph.add_edge(\"call_model\", \"write_memory\")\n",
    "graph.add_edge(\"write_memory\", END)\n",
    "\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Yoona\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Yoona! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Yoona\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I love 2nd generation k-pop group and AI enthusiast.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems like you mentioned your interests in 2nd generation K-pop groups and artificial intelligence. If you have any specific questions or if you'd like to discuss these topics further, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "input_messages = [HumanMessage(content=\"I love 2nd generation k-pop group and AI enthusiast.\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'user_name': 'Yoona',\n",
       "  'interests': ['2nd generation K-pop groups', 'AI enthusiast']},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2025-01-01T10:26:01.021240+00:00',\n",
       " 'updated_at': '2025-01-01T10:26:01.021241+00:00'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What book about AI do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I recommend the book \"Life 3.0: Being Human in the Age of Artificial Intelligence\" by Max Tegmark. It explores the potential impact of artificial intelligence on humanity and society, covering various aspects of AI development and its implications. Given your interest in AI, I believe you will find it both informative and thought-provoking.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "input_messages = [HumanMessage(content=\"What book about AI do you recommend for me?\")]\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
